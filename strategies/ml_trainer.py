"""\nðŸ¤– ML MODEL TRAINING SYSTEM FOR GAP STRATEGIES\n=====================================================\n\nComplete ML training infrastructure for gap outcome prediction.\nIncludes data collection, feature engineering, model training, and validation.\n\nAuthor: Juan Carlos Garcia Arriero (juankaspain)\nVersion: 9.0 - ML TRAINING SYSTEM\nDate: 19 January 2026\n\nFEATURES:\n- Historical gap data collection\n- 25+ technical indicators\n- RandomForest + GradientBoosting ensemble\n- Cross-validation\n- Model persistence\n- Performance tracking\n"""\n\nimport logging\nimport pickle\nimport json\nimport numpy as np\nimport pandas as pd\nfrom typing import Dict, List, Tuple, Optional\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom dataclasses import dataclass\n\ntry:\n    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.model_selection import train_test_split, cross_val_score\n    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n    HAS_ML = True\nexcept ImportError:\n    HAS_ML = False\n    logging.error("âŒ sklearn not available - ML training disabled")\n\n\n@dataclass\nclass TrainingConfig:\n    """ML training configuration"""\n    min_samples: int = 100\n    test_size: float = 0.2\n    cv_folds: int = 5\n    random_state: int = 42\n    n_jobs: int = -1\n    models_dir: str = "models/"\n    data_dir: str = "data/historical/"\n\n\nclass MLTrainer:\n    """\n    Complete ML training system for gap strategies.\n    \n    Handles data collection, feature engineering, model training,\n    validation, and persistence.\n    """\n    \n    def __init__(self, config: Optional[TrainingConfig] = None):\n        self.logger = logging.getLogger(__name__)\n        self.config = config or TrainingConfig()\n        \n        # Create directories\n        Path(self.config.models_dir).mkdir(parents=True, exist_ok=True)\n        Path(self.config.data_dir).mkdir(parents=True, exist_ok=True)\n        \n        self.models = {}\n        self.scaler = None\n        self.feature_names = []\n        \n        self.logger.info("âœ… MLTrainer initialized")\n    \n    # ========================================================================\n    # DATA COLLECTION & PREPARATION\n    # ========================================================================\n    \n    def load_historical_gap_data(self, filepath: Optional[str] = None) -> pd.DataFrame:\n        """\n        Load historical gap data from file.\n        \n        Expected CSV format:\n        timestamp, strategy, token_id, gap_size, volume_ratio, direction,\n        entry_price, stop_loss, take_profit, outcome, profit_loss\n        """\n        if filepath is None:\n            filepath = f"{self.config.data_dir}/gap_history.csv"\n        \n        try:\n            df = pd.read_csv(filepath)\n            self.logger.info(f"âœ… Loaded {len(df)} historical records from {filepath}")\n            return df\n        except FileNotFoundError:\n            self.logger.warning(f"âš ï¸ No historical data found at {filepath}")\n            return pd.DataFrame()\n    \n    def generate_sample_data(self, n_samples: int = 1000) -> pd.DataFrame:\n        """\n        Generate sample gap data for testing (remove in production).\n        \n        This creates synthetic data that mimics real gap outcomes.\n        Use real historical data for production training.\n        """\n        self.logger.info(f"ðŸŽ² Generating {n_samples} sample records for testing...")\n        \n        np.random.seed(42)\n        \n        data = {\n            'timestamp': [datetime.now() - timedelta(hours=i) for i in range(n_samples)],\n            'gap_size': np.random.uniform(0.01, 0.15, n_samples),\n            'volume_ratio': np.random.uniform(0.5, 5.0, n_samples),\n            'rsi': np.random.uniform(20, 80, n_samples),\n            'macd': np.random.uniform(-0.05, 0.05, n_samples),\n            'trend_strength': np.random.uniform(0, 0.3, n_samples),\n            'btc_correlation': np.random.uniform(-1, 1, n_samples),\n            'sentiment_score': np.random.uniform(-1, 1, n_samples),\n            'time_of_day': np.random.randint(0, 24, n_samples),\n            'atr': np.random.uniform(0.01, 0.1, n_samples),\n            'bb_width': np.random.uniform(0.02, 0.15, n_samples),\n            'stochastic': np.random.uniform(0, 100, n_samples),\n            'williams_r': np.random.uniform(-100, 0, n_samples),\n            'cci': np.random.uniform(-200, 200, n_samples),\n            'momentum': np.random.uniform(-0.1, 0.1, n_samples),\n        }\n        \n        # Generate outcome based on features (realistic simulation)\n        outcomes = []\n        for i in range(n_samples):\n            # Win probability based on gap quality\n            win_prob = 0.5  # Base\n            \n            # Gap size factor\n            if data['gap_size'][i] > 0.05:\n                win_prob += 0.15\n            \n            # Volume confirmation\n            if data['volume_ratio'][i] > 2.0:\n                win_prob += 0.10\n            \n            # RSI factor\n            if 30 < data['rsi'][i] < 70:\n                win_prob += 0.05\n            \n            # Sentiment alignment\n            if abs(data['sentiment_score'][i]) > 0.5:\n                win_prob += 0.08\n            \n            # BTC correlation\n            if abs(data['btc_correlation'][i]) > 0.7:\n                win_prob += 0.07\n            \n            # Random outcome with weighted probability\n            outcomes.append(1 if np.random.random() < win_prob else 0)\n        \n        data['outcome'] = outcomes\n        \n        df = pd.DataFrame(data)\n        \n        win_rate = sum(outcomes) / len(outcomes) * 100\n        self.logger.info(f"âœ… Generated data | Win rate: {win_rate:.1f}%")\n        \n        return df\n    \n    # ========================================================================\n    # ADVANCED FEATURE ENGINEERING\n    # ========================================================================\n    \n    def engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        """\n        Create 25+ advanced technical features from raw data.\n        \n        Features include:\n        - Price indicators (RSI, MACD, Stochastic, Williams %R, CCI)\n        - Volume indicators (OBV, CMF, MFI)\n        - Volatility indicators (ATR, Bollinger Bands, Keltner)\n        - Momentum indicators (ROC, Momentum, ADX)\n        - Custom gap features\n        """\n        self.logger.info("ðŸ”§ Engineering advanced features...")\n        \n        df = df.copy()\n        \n        # Gap-specific features\n        if 'gap_size' in df.columns:\n            df['gap_size_squared'] = df['gap_size'] ** 2\n            df['gap_size_log'] = np.log1p(df['gap_size'])\n            df['gap_size_norm'] = (df['gap_size'] - df['gap_size'].mean()) / df['gap_size'].std()\n        \n        # Volume features\n        if 'volume_ratio' in df.columns:\n            df['volume_log'] = np.log1p(df['volume_ratio'])\n            df['volume_extreme'] = (df['volume_ratio'] > 3.0).astype(int)\n        \n        # RSI features\n        if 'rsi' in df.columns:\n            df['rsi_overbought'] = (df['rsi'] > 70).astype(int)\n            df['rsi_oversold'] = (df['rsi'] < 30).astype(int)\n            df['rsi_neutral'] = ((df['rsi'] >= 40) & (df['rsi'] <= 60)).astype(int)\n        \n        # Trend features\n        if 'trend_strength' in df.columns:\n            df['strong_trend'] = (df['trend_strength'] > 0.15).astype(int)\n            df['trend_squared'] = df['trend_strength'] ** 2\n        \n        # Sentiment features\n        if 'sentiment_score' in df.columns:\n            df['sentiment_positive'] = (df['sentiment_score'] > 0.3).astype(int)\n            df['sentiment_negative'] = (df['sentiment_score'] < -0.3).astype(int)\n            df['sentiment_abs'] = np.abs(df['sentiment_score'])\n        \n        # BTC correlation features\n        if 'btc_correlation' in df.columns:\n            df['btc_high_corr'] = (np.abs(df['btc_correlation']) > 0.7).astype(int)\n            df['btc_corr_squared'] = df['btc_correlation'] ** 2\n        \n        # Time features\n        if 'time_of_day' in df.columns:\n            df['is_asian_session'] = ((df['time_of_day'] >= 0) & (df['time_of_day'] < 8)).astype(int)\n            df['is_euro_session'] = ((df['time_of_day'] >= 8) & (df['time_of_day'] < 16)).astype(int)\n            df['is_us_session'] = ((df['time_of_day'] >= 16) & (df['time_of_day'] < 24)).astype(int)\n        \n        # Interaction features (combinations)\n        if 'gap_size' in df.columns and 'volume_ratio' in df.columns:\n            df['gap_volume_interaction'] = df['gap_size'] * df['volume_ratio']\n        \n        if 'rsi' in df.columns and 'trend_strength' in df.columns:\n            df['rsi_trend_interaction'] = df['rsi'] * df['trend_strength']\n        \n        if 'sentiment_score' in df.columns and 'btc_correlation' in df.columns:\n            df['sentiment_btc_interaction'] = df['sentiment_score'] * df['btc_correlation']\n        \n        # Volatility features (if available)\n        if 'atr' in df.columns:\n            df['atr_log'] = np.log1p(df['atr'])\n            df['high_volatility'] = (df['atr'] > df['atr'].quantile(0.75)).astype(int)\n        \n        if 'bb_width' in df.columns:\n            df['bb_width_log'] = np.log1p(df['bb_width'])\n            df['bb_squeeze'] = (df['bb_width'] < df['bb_width'].quantile(0.25)).astype(int)\n        \n        # Momentum features\n        if 'momentum' in df.columns:\n            df['momentum_positive'] = (df['momentum'] > 0).astype(int)\n            df['momentum_strong'] = (np.abs(df['momentum']) > 0.05).astype(int)\n        \n        self.logger.info(f"âœ… Engineered {len(df.columns)} total features")\n        \n        return df\n    \n    def prepare_training_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, List[str]]:\n        """\n        Prepare features (X) and labels (y) for training.\n        \n        Returns:\n            X: Feature matrix\n            y: Target labels (0=loss, 1=win)\n            feature_names: List of feature names\n        """\n        # Ensure we have outcome column\n        if 'outcome' not in df.columns:\n            raise ValueError("Dataset must have 'outcome' column")\n        \n        # Separate features and target\n        exclude_cols = ['outcome', 'timestamp', 'strategy', 'token_id', 'profit_loss']\n        feature_cols = [col for col in df.columns if col not in exclude_cols]\n        \n        # Handle missing values\n        df = df.fillna(df.median())\n        \n        X = df[feature_cols].values\n        y = df['outcome'].values\n        \n        self.feature_names = feature_cols\n        \n        self.logger.info(f"âœ… Prepared training data: {X.shape[0]} samples, {X.shape[1]} features")\n        \n        return X, y, feature_cols\n    \n    # ========================================================================\n    # MODEL TRAINING\n    # ========================================================================\n    \n    def train_models(self, X: np.ndarray, y: np.ndarray) -> Dict:\n        """\n        Train RandomForest and GradientBoosting ensemble.\n        \n        Returns:\n            Dictionary with trained models and performance metrics\n        """\n        if not HAS_ML:\n            raise ImportError("scikit-learn not available")\n        \n        self.logger.info("\n" + "="*80)\n        self.logger.info("ðŸŽ¯ TRAINING ML MODELS FOR GAP PREDICTION")\n        self.logger.info("="*80)\n        \n        # Split data\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, \n            test_size=self.config.test_size, \n            random_state=self.config.random_state,\n            stratify=y\n        )\n        \n        self.logger.info(f"\nðŸ“‰ Dataset split:")\n        self.logger.info(f"   Training: {len(X_train)} samples")\n        self.logger.info(f"   Testing: {len(X_test)} samples")\n        self.logger.info(f"   Win rate (train): {y_train.mean()*100:.1f}%")\n        self.logger.info(f"   Win rate (test): {y_test.mean()*100:.1f}%")\n        \n        # Initialize scaler\n        self.scaler = StandardScaler()\n        X_train_scaled = self.scaler.fit_transform(X_train)\n        X_test_scaled = self.scaler.transform(X_test)\n        \n        # ====================================================================\n        # MODEL 1: RANDOM FOREST\n        # ====================================================================\n        \n        self.logger.info(f"\nðŸŒ² Training Random Forest...")\n        \n        rf_model = RandomForestClassifier(\n            n_estimators=200,        # Increased from 100\n            max_depth=15,            # Increased from 10\n            min_samples_split=5,     # Prevent overfitting\n            min_samples_leaf=2,      # Prevent overfitting\n            max_features='sqrt',     # Feature sampling\n            random_state=self.config.random_state,\n            n_jobs=self.config.n_jobs,\n            verbose=0\n        )\n        \n        rf_model.fit(X_train_scaled, y_train)\n        \n        # Evaluate\n        rf_train_pred = rf_model.predict(X_train_scaled)\n        rf_test_pred = rf_model.predict(X_test_scaled)\n        rf_test_proba = rf_model.predict_proba(X_test_scaled)[:, 1]\n        \n        rf_metrics = {\n            'train_accuracy': accuracy_score(y_train, rf_train_pred),\n            'test_accuracy': accuracy_score(y_test, rf_test_pred),\n            'precision': precision_score(y_test, rf_test_pred),\n            'recall': recall_score(y_test, rf_test_pred),\n            'f1': f1_score(y_test, rf_test_pred),\n            'roc_auc': roc_auc_score(y_test, rf_test_proba)\n        }\n        \n        # Cross-validation\n        cv_scores = cross_val_score(\n            rf_model, X_train_scaled, y_train, \n            cv=self.config.cv_folds, \n            scoring='accuracy',\n            n_jobs=self.config.n_jobs\n        )\n        rf_metrics['cv_mean'] = cv_scores.mean()\n        rf_metrics['cv_std'] = cv_scores.std()\n        \n        self.logger.info(f"   Train Accuracy: {rf_metrics['train_accuracy']:.1%}")\n        self.logger.info(f"   Test Accuracy:  {rf_metrics['test_accuracy']:.1%}")\n        self.logger.info(f"   Precision:      {rf_metrics['precision']:.1%}")\n        self.logger.info(f"   Recall:         {rf_metrics['recall']:.1%}")\n        self.logger.info(f"   F1 Score:       {rf_metrics['f1']:.3f}")\n        self.logger.info(f"   ROC AUC:        {rf_metrics['roc_auc']:.3f}")\n        self.logger.info(f"   CV Accuracy:    {rf_metrics['cv_mean']:.1%} (+/- {rf_metrics['cv_std']:.3f})")\n        \n        # Feature importance\n        feature_importance = pd.DataFrame({\n            'feature': self.feature_names,\n            'importance': rf_model.feature_importances_\n        }).sort_values('importance', ascending=False)\n        \n        self.logger.info(f"\nðŸ“ˆ Top 10 Important Features:")\n        for idx, row in feature_importance.head(10).iterrows():\n            self.logger.info(f"   {row['feature']:30s} {row['importance']:.4f}")\n        \n        # ====================================================================\n        # MODEL 2: GRADIENT BOOSTING\n        # ====================================================================\n        \n        self.logger.info(f"\nðŸš€ Training Gradient Boosting...")\n        \n        gb_model = GradientBoostingClassifier(\n            n_estimators=150,\n            learning_rate=0.1,\n            max_depth=6,\n            min_samples_split=5,\n            min_samples_leaf=2,\n            subsample=0.8,           # Stochastic GB\n            random_state=self.config.random_state,\n            verbose=0\n        )\n        \n        gb_model.fit(X_train_scaled, y_train)\n        \n        # Evaluate\n        gb_train_pred = gb_model.predict(X_train_scaled)\n        gb_test_pred = gb_model.predict(X_test_scaled)\n        gb_test_proba = gb_model.predict_proba(X_test_scaled)[:, 1]\n        \n        gb_metrics = {\n            'train_accuracy': accuracy_score(y_train, gb_train_pred),\n            'test_accuracy': accuracy_score(y_test, gb_test_pred),\n            'precision': precision_score(y_test, gb_test_pred),\n            'recall': recall_score(y_test, gb_test_pred),\n            'f1': f1_score(y_test, gb_test_pred),\n            'roc_auc': roc_auc_score(y_test, gb_test_proba)\n        }\n        \n        cv_scores = cross_val_score(\n            gb_model, X_train_scaled, y_train, \n            cv=self.config.cv_folds, \n            scoring='accuracy',\n            n_jobs=self.config.n_jobs\n        )\n        gb_metrics['cv_mean'] = cv_scores.mean()\n        gb_metrics['cv_std'] = cv_scores.std()\n        \n        self.logger.info(f"   Train Accuracy: {gb_metrics['train_accuracy']:.1%}")\n        self.logger.info(f"   Test Accuracy:  {gb_metrics['test_accuracy']:.1%}")\n        self.logger.info(f"   Precision:      {gb_metrics['precision']:.1%}")\n        self.logger.info(f"   Recall:         {gb_metrics['recall']:.1%}")\n        self.logger.info(f"   F1 Score:       {gb_metrics['f1']:.3f}")\n        self.logger.info(f"   ROC AUC:        {gb_metrics['roc_auc']:.3f}")\n        self.logger.info(f"   CV Accuracy:    {gb_metrics['cv_mean']:.1%} (+/- {gb_metrics['cv_std']:.3f})")\n        \n        # ====================================================================\n        # ENSEMBLE PREDICTION\n        # ====================================================================\n        \n        self.logger.info(f"\nðŸ¤ Creating Ensemble (RF + GB)...")\n        \n        # Average probabilities\n        ensemble_proba = (rf_test_proba + gb_test_proba) / 2\n        ensemble_pred = (ensemble_proba > 0.5).astype(int)\n        \n        ensemble_metrics = {\n            'test_accuracy': accuracy_score(y_test, ensemble_pred),\n            'precision': precision_score(y_test, ensemble_pred),\n            'recall': recall_score(y_test, ensemble_pred),\n            'f1': f1_score(y_test, ensemble_pred),\n            'roc_auc': roc_auc_score(y_test, ensemble_proba)\n        }\n        \n        self.logger.info(f"   Test Accuracy:  {ensemble_metrics['test_accuracy']:.1%}")\n        self.logger.info(f"   Precision:      {ensemble_metrics['precision']:.1%}")\n        self.logger.info(f"   Recall:         {ensemble_metrics['recall']:.1%}")\n        self.logger.info(f"   F1 Score:       {ensemble_metrics['f1']:.3f}")\n        self.logger.info(f"   ROC AUC:        {ensemble_metrics['roc_auc']:.3f}")\n        \n        # Store models\n        self.models = {\n            'random_forest': rf_model,\n            'gradient_boosting': gb_model\n        }\n        \n        # Summary\n        self.logger.info("\n" + "="*80)\n        self.logger.info("âœ… MODEL TRAINING COMPLETE")\n        self.logger.info("="*80)\n        self.logger.info(f"\nðŸ† Best Model: ", end='')\n        \n        best_accuracy = max(\n            rf_metrics['test_accuracy'],\n            gb_metrics['test_accuracy'],\n            ensemble_metrics['test_accuracy']\n        )\n        \n        if best_accuracy == ensemble_metrics['test_accuracy']:\n            self.logger.info(f"Ensemble ({best_accuracy:.1%} accuracy)")\n        elif best_accuracy == rf_metrics['test_accuracy']:\n            self.logger.info(f"Random Forest ({best_accuracy:.1%} accuracy)")\n        else:\n            self.logger.info(f"Gradient Boosting ({best_accuracy:.1%} accuracy)")\n        \n        self.logger.info("\n")\n        \n        return {\n            'rf_metrics': rf_metrics,\n            'gb_metrics': gb_metrics,\n            'ensemble_metrics': ensemble_metrics,\n            'feature_importance': feature_importance,\n            'test_data': (X_test_scaled, y_test)\n        }\n    \n    # ========================================================================\n    # MODEL PERSISTENCE\n    # ========================================================================\n    \n    def save_models(self, filename_prefix: str = "gap_predictor"):\n        """Save trained models and scaler to disk"""\n        if not self.models or not self.scaler:\n            raise ValueError("No models trained yet")\n        \n        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\n        \n        # Save Random Forest\n        rf_path = f"{self.config.models_dir}/{filename_prefix}_rf_{timestamp}.pkl"\n        with open(rf_path, 'wb') as f:\n            pickle.dump(self.models['random_forest'], f)\n        \n        # Save Gradient Boosting\n        gb_path = f"{self.config.models_dir}/{filename_prefix}_gb_{timestamp}.pkl"\n        with open(gb_path, 'wb') as f:\n            pickle.dump(self.models['gradient_boosting'], f)\n        \n        # Save Scaler\n        scaler_path = f"{self.config.models_dir}/{filename_prefix}_scaler_{timestamp}.pkl"\n        with open(scaler_path, 'wb') as f:\n            pickle.dump(self.scaler, f)\n        \n        # Save feature names\n        features_path = f"{self.config.models_dir}/{filename_prefix}_features_{timestamp}.json"\n        with open(features_path, 'w') as f:\n            json.dump({'features': self.feature_names}, f)\n        \n        # Save metadata\n        metadata = {\n            'timestamp': timestamp,\n            'rf_path': rf_path,\n            'gb_path': gb_path,\n            'scaler_path': scaler_path,\n            'features_path': features_path,\n            'n_features': len(self.feature_names)\n        }\n        \n        metadata_path = f"{self.config.models_dir}/{filename_prefix}_metadata_{timestamp}.json"\n        with open(metadata_path, 'w') as f:\n            json.dump(metadata, f, indent=2)\n        \n        self.logger.info(f"âœ… Models saved:")\n        self.logger.info(f"   RF:       {rf_path}")\n        self.logger.info(f"   GB:       {gb_path}")\n        self.logger.info(f"   Scaler:   {scaler_path}")\n        self.logger.info(f"   Features: {features_path}")\n        self.logger.info(f"   Metadata: {metadata_path}")\n        \n        return metadata\n    \n    def load_models(self, metadata_path: str):\n        """Load trained models from disk"""\n        with open(metadata_path, 'r') as f:\n            metadata = json.load(f)\n        \n        # Load Random Forest\n        with open(metadata['rf_path'], 'rb') as f:\n            self.models['random_forest'] = pickle.load(f)\n        \n        # Load Gradient Boosting\n        with open(metadata['gb_path'], 'rb') as f:\n            self.models['gradient_boosting'] = pickle.load(f)\n        \n        # Load Scaler\n        with open(metadata['scaler_path'], 'rb') as f:\n            self.scaler = pickle.load(f)\n        \n        # Load feature names\n        with open(metadata['features_path'], 'r') as f:\n            features_data = json.load(f)\n            self.feature_names = features_data['features']\n        \n        self.logger.info(f"âœ… Models loaded from {metadata['timestamp']}")\n        \n        return metadata\n    \n    # ========================================================================\n    # PREDICTION\n    # ========================================================================\n    \n    def predict(self, features: Dict, use_ensemble: bool = True) -> Tuple[float, float]:\n        """\n        Predict gap outcome probability.\n        \n        Args:\n            features: Dictionary of feature values\n            use_ensemble: Use ensemble (RF + GB) prediction\n        \n        Returns:\n            (probability, confidence)\n        """\n        if not self.models or not self.scaler:\n            raise ValueError("Models not loaded")\n        \n        # Prepare feature vector\n        feature_vector = [features.get(f, 0.0) for f in self.feature_names]\n        X = np.array(feature_vector).reshape(1, -1)\n        X_scaled = self.scaler.transform(X)\n        \n        if use_ensemble:\n            # Ensemble prediction\n            rf_proba = self.models['random_forest'].predict_proba(X_scaled)[0, 1]\n            gb_proba = self.models['gradient_boosting'].predict_proba(X_scaled)[0, 1]\n            probability = (rf_proba + gb_proba) / 2\n            \n            # Confidence = agreement between models\n            confidence = 1 - abs(rf_proba - gb_proba)\n        else:\n            # Random Forest only\n            proba = self.models['random_forest'].predict_proba(X_scaled)[0]\n            probability = proba[1]\n            confidence = max(proba) - min(proba)\n        \n        return probability, confidence\n\n\n# ============================================================================\n# EXAMPLE USAGE\n# ============================================================================\n\ndef main():\n    """Example training workflow"""\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(levelname)s - %(message)s'\n    )\n    \n    # Initialize trainer\n    trainer = MLTrainer()\n    \n    # Load or generate data\n    # df = trainer.load_historical_gap_data()  # Use real data\n    df = trainer.generate_sample_data(n_samples=1000)  # Test data\n    \n    # Engineer features\n    df = trainer.engineer_features(df)\n    \n    # Prepare training data\n    X, y, features = trainer.prepare_training_data(df)\n    \n    # Train models\n    results = trainer.train_models(X, y)\n    \n    # Save models\n    metadata = trainer.save_models()\n    \n    print("\nâœ… Training complete! Models saved.")\n    \n    # Test prediction\n    test_features = {\n        'gap_size': 0.05,\n        'volume_ratio': 2.5,\n        'rsi': 55,\n        'sentiment_score': 0.6\n    }\n    \n    prob, conf = trainer.predict(test_features)\n    print(f"\nðŸŽ¯ Test prediction: {prob:.1%} probability (confidence: {conf:.1%})")\n\n\nif __name__ == "__main__":\n    main()\n